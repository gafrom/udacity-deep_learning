{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Assignment 3\n",
    "=============\n",
    "<span style=\"color: lightsteelblue;\">Udacity university | Deep Learning</span>\n",
    "\n",
    "Previously in `assignment_2.ipynb`, we trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom libraries\n",
    "\n",
    "# To plot charts\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to problem 1\n",
    "\n",
    "Let's start playing. We start with logistic model and then move on to deep network.\n",
    "\n",
    "### Logistic model\n",
    "\n",
    "First we tweak the loss by adding Euclidean norm.\n",
    "\n",
    "Below we override `loss` function keeping `original_loss` intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  original_loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  loss = original_loss + beta * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run logistic model with tweaked `loss` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 26.049599\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 6.1%\n",
      "Minibatch loss at step 500: 2.425125\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 1000: 1.736043\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1500: 0.892354\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 2000: 0.832689\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2500: 0.804534\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.775876\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 88.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result the accuracy increased by 2 points - from `86.9%` to `88.9%`! Great stuff those Euclidean norms are, which penalize big weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "\n",
    "The same here - we tweak the loss by adding Euclidean norm.\n",
    "\n",
    "Below we override `loss` function keeping `original_loss` intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Hidden layer variables\n",
    "  hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_nodes]))\n",
    "\n",
    "  # Variables\n",
    "  weights = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  hidden_layer_train = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "  logits = tf.matmul(hidden_layer_train, weights) + biases\n",
    "  original_loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  loss = original_loss + beta * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(hidden_weights))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  hidden_layer_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_valid, weights) + biases)\n",
    "\n",
    "  hidden_layer_test= tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_test, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how new `loss` function works with neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 667.363647\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 26.7%\n",
      "Minibatch loss at step 500: 201.127304\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1000: 116.982765\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1500: 68.475349\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2000: 41.329937\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 25.155510\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3000: 15.517172\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see? Geee, the accuracy is astonishing :)\n",
    "\n",
    "Let us find the best `beta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with beta: 0.0001\n",
      "Initialized with beta: 0.000125892541179\n",
      "Initialized with beta: 0.000158489319246\n",
      "Initialized with beta: 0.000199526231497\n",
      "Initialized with beta: 0.000251188643151\n",
      "Initialized with beta: 0.000316227766017\n",
      "Initialized with beta: 0.000398107170553\n",
      "Initialized with beta: 0.000501187233627\n",
      "Initialized with beta: 0.00063095734448\n",
      "Initialized with beta: 0.000794328234724\n",
      "Initialized with beta: 0.001\n",
      "Initialized with beta: 0.00125892541179\n",
      "Initialized with beta: 0.00158489319246\n",
      "Initialized with beta: 0.00199526231497\n",
      "Initialized with beta: 0.00251188643151\n",
      "Initialized with beta: 0.00316227766017\n",
      "Initialized with beta: 0.00398107170553\n",
      "Initialized with beta: 0.00501187233627\n",
      "Initialized with beta: 0.0063095734448\n",
      "Initialized with beta: 0.00794328234724\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "beta_values_to_try = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracies = []\n",
    "\n",
    "for beta_value in beta_values_to_try:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized with beta: %s\" % beta_value)\n",
    "    for step in range(num_steps):\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: beta_value}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracies.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b355710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(beta_values_to_try, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the best value of `beta` is somewhere starting around `10e-3`. I pick `0.001`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
