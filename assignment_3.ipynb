{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Assignment 3\n",
    "=============\n",
    "<span style=\"color: lightsteelblue;\">Udacity university | Deep Learning</span>\n",
    "\n",
    "Previously in `assignment_2.ipynb`, we trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom libraries\n",
    "\n",
    "# To plot charts\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to problem 2\n",
    "\n",
    "Let's start playing. We start with logistic model and then move on to deep network.\n",
    "\n",
    "### Logistic model\n",
    "\n",
    "First we tweak the loss by adding Euclidean norm.\n",
    "\n",
    "Below we override `loss` function keeping `original_loss` intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "\n",
    "The same here - we tweak the loss by adding Euclidean norm.\n",
    "\n",
    "Below we override `loss` function keeping `original_loss` intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Hidden layer variables\n",
    "  hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([num_nodes]))\n",
    "\n",
    "  # Variables\n",
    "  weights = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  hidden_layer_train = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "  logits = tf.matmul(hidden_layer_train, weights) + biases\n",
    "  original_loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  loss = original_loss + beta * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(hidden_weights))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  hidden_layer_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_valid, weights) + biases)\n",
    "\n",
    "  hidden_layer_test= tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_test, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how new `loss` function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 667.363647\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 26.7%\n",
      "Minibatch loss at step 500: 201.127304\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1000: 116.982765\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1500: 68.475349\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2000: 41.329937\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 25.155510\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3000: 15.517172\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see? Geee, the accuracy is astonishing :)\n",
    "\n",
    "Let us find the best `beta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1501\n",
    "beta_values_to_try = [pow(10, i) for i in np.arange(-2.4, -1.8, 0.1)]\n",
    "accuracies = []\n",
    "\n",
    "for beta_value in beta_values_to_try:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: beta_value}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracies.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92.060000000000002, 91.540000000000006, 91.290000000000006, 90.819999999999993, 90.609999999999999, 90.030000000000001] [0.0039810717055349734, 0.0050118723362727246, 0.0063095734448019363, 0.0079432823472428207, 0.010000000000000011, 0.012589254117941687]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113e0f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(accuracies, beta_values_to_try)\n",
    "plt.plot(beta_values_to_try, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
