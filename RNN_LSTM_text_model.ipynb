{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN LSTM for text classification\n",
    "=============\n",
    "<span style=\"color: lightsteelblue;\">Deep Learning</span>\n",
    "\n",
    "The goal of this notebook is to train recurrent neural network with LSTM cell for the purpose of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input size: 6508 words\n",
      "\n",
      "Number of business articles is 3. For example: \n",
      "Индийский слон Кофейный рынок лихорадит. К плохим новостям из Южной Америки, гуляющим уже несколько недель, на днях добавились тревожные звонки из Индии, которая является веду...\n",
      "Number of culture articles is 6. For example: \n",
      "Если погуглить рекомендательные подборки семейных саг, то на разных литературных сайтах в списке из условных 10 позиций непременно окажется «Сага о Форсайтах» и, надо думать, ...\n",
      "Number of sport articles is 4. For example: \n",
      "Сами себе чемпионы Среди всех участников чемпионата мира на этот раз было лишь 19 обладателей паспорта в бордовой корочке. Хотя формально ни одного россиянина на турнире не бы...\n"
     ]
    }
   ],
   "source": [
    "def clean(string):\n",
    "  string = re.sub(r\"[^А-Яа-я0-9(),!?]\", \" \", string)\n",
    "  string = re.sub(r\",\", \" , \", string)\n",
    "  string = re.sub(r\"!\", \" ! \", string)\n",
    "  string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "  string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "  string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "  string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "  return string.strip().lower()\n",
    "\n",
    "def read_data(folder):\n",
    "  data = {}\n",
    "  data_files = os.listdir(folder)\n",
    "  for data_file in data_files:\n",
    "    with open(os.path.join(folder,data_file)) as f:\n",
    "      articles = []\n",
    "      for line in f:\n",
    "        articles.append(line)\n",
    "    data[data_file] = articles\n",
    "\n",
    "  return data\n",
    "\n",
    "data = read_data('articles')\n",
    "num_classes = len(data)\n",
    "\n",
    "total_articles_count = 0\n",
    "labels = []\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for label_index, label in enumerate(data):\n",
    "  labels.append(label)\n",
    "\n",
    "  for article in data[label]:\n",
    "    sentences = re.split('[\\.]', article)\n",
    "    for sentence in sentences:\n",
    "      words = clean(sentence).split(' ')\n",
    "\n",
    "      data_x = data_x + words\n",
    "      data_y.append(label_index)\n",
    "\n",
    "data_size = len(data_x)\n",
    "\n",
    "print('Total input size: %d words\\n' % data_size)\n",
    "\n",
    "for label in data:\n",
    "  excerpt = data[label][0][:175] + '...'\n",
    "  print('Number of %s articles is %d. For example: \\n%s' % (label, len(data[label]), excerpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNK', 162], (',', 627), ('в', 227), ('и', 197), ('на', 110), ('не', 86), ('что', 67), ('с', 57), ('это', 55), ('как', 53)]\n",
      "684\n",
      "и\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 2600\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(data_x)\n",
    "\n",
    "train_size = round(data_size * 0.95)\n",
    "valid_size = data_size - train_size\n",
    "\n",
    "train_data   = np.array(data[:train_size])\n",
    "train_labels = np.array(data_y[:train_size])\n",
    "valid_data   = np.array(data[train_size:])\n",
    "valid_labels = np.array(data_y[train_size:])\n",
    "\n",
    "print(count[:10])\n",
    "print(dictionary['слон'])\n",
    "print(reverse_dictionary[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['америки', 'в', '10', 'пойдут', 'общепита', '2016', 'сейчас', 'основания', 'компании', 'на', 'от', 'это', 'содержится', 'авиастроительной', 'го', 'и', 'в', 'материальных', 'являются', 'разделу', 'сухого', 'сага', 'форстер', 'ее', ',', 'материального', 'форсайтах', '', 'из', 'когда', 'цикл', 'небольшого', 'и', ',', 'с', 'хх', 'живете', 'еще', 'нет', 'и', 'тирана', 'ни', 'побороть', 'и', 'было', 'наконец', 'няньки', 'социальные', 'инвалиды', ',', ',', 'что', 'можем', 'их', 'зрителю', 'этой', 'мило', ',', 'шубенков', 'мария', ',', 'вот', 'стал', 'эстафете']\n",
      "['тревожные', 'а', 'йорка', 'этой', 'подорожал', 'кофе', 'текущего', 'анонимности', 'экономическая', 'больше', 'что', 'конечно', 'чашке', 'самолете', 'новак', ',', 'самолетов', 'финансовом', 'от', ',', 'и', 'из', 'такого', ',', 'них', 'но', 'любовь', 'не', 'прежний', 'то', ',', 'ощущениям', 'долго', ',', ',', ',', 'городе', 'таким', 'разному', 'у', 'ирэн', 'что', 'любви', 'умеют', 'уэллера', 'предубеждение', ',', 'нее', 'сейчас', 'году', 'как', 'феллоуз', 'твид', 'т', 'дворецкий', 'какому', 'его', 'подводя', 'по', 'секторе', 'два', 'и', 'и', 'UNK']\n",
      "['так']\n",
      "['UNK']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, self._text[self._cursor[b]]] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def words(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  words back into its (most likely) word representation.\"\"\"\n",
    "  return [reverse_dictionary[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  for b in batches:\n",
    "    s = [word for word in words(b)]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_data, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_data, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.float32, shape=[None, vocabulary_size])\n",
    "  train_labels = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      labels=train_labels,\n",
    "      logits=logits\n",
    "    ))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'g', 'r', 'c', 'c', 'a', 'o', 'o', 'r', 'k']\n",
      "['m', 'g', 'r']\n",
      "['c', 'c', 'a', 'o', 'o', 'r', 'k']\n"
     ]
    }
   ],
   "source": [
    "alph = \"abcdefghijklmnoprst\"\n",
    "chars = []\n",
    "\n",
    "for _ in range(10):\n",
    "  r_char = random.choice(list(alph))\n",
    "  chars.append(r_char)\n",
    "\n",
    "print(chars)\n",
    "print(chars[:3])\n",
    "print(chars[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
